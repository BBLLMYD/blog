## 关于【程序 = 数据结构 + 算法】的反思

**程序 = 数据结构 + 算法**，这个公式是在学习软件工程专业时的第一句话，或者说是认知基础，而且后面的课程也基本都是在这上展开的。
数据结构是组织和存储数据，而算法是处理数据的步骤和方法。所有的基础类组件以及复杂业务问题的解决，都是由此作为基础支撑。

直到工作了很多年，我仍然认为这个公式是正确的，且随着多年工作经验的累积，每想起来这个公式，**愈发认为这个公式的描述是十分准确并且干练的。
**

但随着接触AI的内容越多，关于这个公式，逐渐产生了不少疑问或怀疑。

因为大部分的**工程类程序员，工作的上下文都是在一个严肃或者相对严肃的场景**
。怎么说严肃呢，比如支付业务，每笔流量直接涉及到金钱，离资损的风险是最近的。所以从业务的各个环节都需要做好一切可能性的应对。针对每一笔流量以及各种可能性都要十分谨慎，做好全面的兜底和保障工作。
对于db的每一次变更都要做好收口以及可解释和追溯的能力。当然实际上不止支付业务，任何业务都有一样的特性，只是离钱更近的业务体感更强烈。

总的来说大多数工程类程序员其实追求的目标，无论是所谓鲁棒性或者扩展性，都是围绕确定性来进行系统建设的。

## 关于AI应用的延伸思考

那为什么在AI的相关的内容的理解和应用的过程中，对上述公式频频产生疑问呢？我认为很重要的一点就是AI的工作机制充满了工程视角的“不确定性”。

很多认知对AI的基础认识是随机+概率，从工程角度来看，概率和随机是AI的原生工具，而在这基础之上有复杂的用来稳定结构的目标优化的方法设计和行为。
其实从某种程度上来说或者生物本身也是一种概率系统。

那在严肃场景AI为基础的智能体应用，到底应该怎么落地？很长一段时间，我都比较保守的认为AI更适合做一些相对保守且稳妥的工作，如果把对数据操作分为
**读/写**的话，我总觉得以读为基础的工作更适合AI，
实际上也确实很多应用都是围绕读的，比如各种知识库理解的对话应用。因为如果让AI做写操作的话，工程视角总会自然认为这种动作充满了不确定性。

其实不确定性，或者说是不稳定性在**AI作为基础主体**的情况下，是客观存在的。那么**问题就来到如何让AI的写动作，变得可控**
，可控的具体包括一下特性比如精准、稳定、可追溯、可解释、可回滚等等。
2026春节火热的千问请大家奶茶的营销活动，是我印象里第一个与C端用户直接交互的有“订单”产生的模式。不同于比如广告业务实际背后也是AI支撑以及数据回写的流程，这种业务里AI的角色并不直接和用户直观的产生交互体验上的对位。

## 对“不确定性”的应用历程

说应用历程有点不合适，应该说我们对于“不确定性”的特质的典型应用。
想说的是“不确定性”虽然未必能被理解，但是不妨碍我们可以对它本身进行使用应用。再谈一谈“不确定性”本身的优势。

- **量子物理**

最典型的**量子力学**是“不确定性”在物理学中的体现。不确定性原理告诉我们，无法同时精确知道一个粒子的位置和动量。
而且量子态本身就是概率性的。
粒子可以处于“同时在这里和那里”的叠加态，直到被测量的那一刻才“随机”地坍缩到一个确定的状态。
**量子计算**这正是应用了这种“不确定性”来制造非常规的能力。量子比特利用叠加态可以同时进行海量的并行计算。
一些算法巧妙地利用量子态的干涉和概率幅，让“正确”结果的概率被放大，而“错误”结果的概率被抵消，
以远超经典计算机的速度解决某些问题。

经典物理和量子物理并不相溶，但是他们确实同时存在且发挥作用。

- **AI**

让生成式AI创作一幅画或写一首诗时，其内部机制是在一个“可能性空间”中进行概率性的采样。
不是去寻找那个“唯一正确”的答案（创造内容来说标准答案本身也不存在），
而是根据它学到的模式，为你生成一个最可能让你满意的同时又带有随机性的结果。

贝叶斯机器学习中，会以置信度来表达对“不确定性”的量化。

强化学习的过程中，会引入大量的随机探索，并且利用这种“不确定”的探索过程中得到的反馈，不断优化自身找到最优解的方法。

“[熵](https://github.com/BBLLMYD/blog/blob/master/blogs/%E6%8A%BD%E8%B1%A1%E4%B9%8B%E4%BA%8E%E2%80%9C%E7%86%B5%E2%80%9D.md)
”也是对不确定性的一种程度量化或者描述。

大量微观层面的“不确定性”互动和累积，最终可能在宏观层面“涌现”出确定的、有序的新结构和规律的过程。

从AI到量子，再到社会和经济，某种角度来说，不确定性是其中的一种原力。

## 正题对于Transformer架构的思考

目前为止，Transformer架构可以认为是大模型构建的事实标准。 Transformer架构的诞生到目前相对成熟的应用延着「论文/理论基础/训练框架迭代/大规模数据验证调试」路径形成。

关于Transformer方方面面的理解都有无限的扩展发散空间，需要有多维全面且融合的知识支撑理解，比如为什么相比其他理论架构为什么Transformer更能落地？到现在还没有其他架构能系统性替代它？架构的演进逻辑有哪些？
细节到注意力机制的理论、并行计算对硬件和算力的极致利用程度、自监督学习和验证等等，可以说需要扎实的数据基础、软件工程、硬件开发效率等等。

在AI产业化或者任何新诞生的产业演化进入到各行业的过程中，偶尔停下来会有不可思议的魔幻感觉。
我觉得大家常常讨论很多的“**涌现**”这个词，是很有魅力很迷人的。或者我更喜欢添加个描述叫它“**自然涌现**”。
“自然”说明它不是被主观强行设计出来的，而是长出来的。或者说是一个整体生命体的迭代步伐。至于步伐去什么方向走向哪里，基本是未知的，但是总是有种种能量去驱动发展。

从原子到分子、从分子到细胞、从细胞到生物、从生物到社会，从生物学角度看每一步都可以看做事一个涌现的过程（类比不一定恰当）

到AI模型，当深度学习模型的规模（参数数量、数据量、计算量）跨越某个阈值后，系统会突然展现出其小规模版本所不具备的、没有被明确目标训练过的复杂且不是被编程进去的高级能力。是系统自己长出来的能力。



> 维基百科：<br>
>
在深度学习中，transformer（直译为“变换器”）是一种基于多头注意力机制的人工神经网络架构，其中文本被转换为称为词元（token）的数值表示，每个词元通过从词嵌入表中查找转换为一个向量。[1]
>
在每一层，每个词元都通过并行的多头注意力机制在上下文窗口的范围内与其他（未屏蔽的）词元进行上下文关联，从而放大关键词元的信号并减弱不太重要的词元，从而按输入数据各部分重要性的不同而分配不同的权重。采用该架构的模型主要用于自然语言处理（NLP）与计算机视觉（CV）领域。





