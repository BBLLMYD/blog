## 对「程序 = 数据结构 + 算法」的被动反思

**程序 = 数据结构 + 算法**，这个公式是在学习软件工程专业时的第一句话，或者说是认知基础，而且后面的课程也基本都是在这上展开的。
数据结构是组织和存储数据，而算法是处理数据的步骤和方法。

直到工作了很多年，我仍然认为这个公式是正确的，且随着多年工作经验的累积，每想起来这个公式，**愈发认为这个公式的描述是十分精准并且干练的。
**

但随着接触AI的内容越多，关于这个公式，逐渐产生了不少疑问或怀疑。

因为大部分的**工程类程序员，工作的上下文都是在一个严肃或者相对严肃的场景**
。怎么说严肃呢，比如支付业务，每笔流量直接涉及到金钱，离资损的风险是最近的。所以从业务的各个环节都需要做好一切可能性的应对。针对每一笔流量以及各种可能性都要十分谨慎，做好全面的兜底和保障工作。
对于db的每一次变更都要做好收口以及可解释和追溯的能力。当然实际上不止支付业务，任何业务都有一样的特性，只是离钱更近的业务对于
**确定性**体感更强烈。

总的来说大多数工程类程序员其实追求的目标，无论是所谓鲁棒性或者扩展性，都是围绕确定性来进行系统建设的。

> 我们知道AI某种程度上相对传统编程是以随机和概率为基础，那么这个公式在AI为基础主体下的编程还适用吗？

我觉得一定程度上，以「程序 = 数据结构 + 算法」公式为基础的编程范式，因为AI的涌现，会大面积受到影响，新的编码范式正在加速形成。

其实可以认为两种范式一直是共存的，从分布上来说只是大多数工业级企业工程开发，都是以传统范式为驱动指导的。
而AI时代的到来将这种情形扩大。

而我想说的是并不是说传统的编码范式收到了挑战，而是随着AI时代的来到新的范式给了我们新的选择和路径。
就像**传统物理和量子物理并不相溶，但它们是共存共生的。**

新的范式还没有统一的标准，我们可以结合元素先将它视作：「程序 = 数据结构 + 算法 + 模型 + 数据分布 + 不确定性控制」

## 关于AI应用的延伸思考

那为什么在AI的相关的内容的理解和应用的过程中，对上述公式频频产生疑问呢？我认为很重要的一点就是AI的工作机制充满了工程视角的“不确定性”。

很多认知对AI的基础认识是随机+概率，从工程角度来看，概率和随机是AI的原生工具，而在这基础之上有复杂的用来稳定结构的目标优化的方法设计和行为。
其实从某种程度上来说或者生物本身也是一种概率系统。

那在严肃场景AI为基础的智能体应用，到底应该怎么落地？很长一段时间，我都比较保守的认为AI更适合做一些相对保守且稳妥的工作，如果把对数据操作分为
**读/写**的话，我总觉得以读为基础的工作更适合AI，
实际上也确实很多应用都是围绕读的，比如各种知识库理解的对话应用。因为如果让AI做写操作的话，工程视角总会自然认为这种动作充满了不确定性。

其实不确定性，或者说是不稳定性在**AI作为基础主体**的情况下，是客观存在的。那么**问题就来到如何让AI的写动作，变得可控**
，可控的具体包括一下特性比如精准、稳定、可追溯、可解释、可回滚等等。
2026春节火热的千问请大家奶茶的营销活动，是我印象里第一个与C端用户直接交互的有“订单”产生的模式。不同于比如广告业务实际背后也是AI支撑以及数据回写的流程，这种业务里AI的角色并不直接和用户直观的产生交互体验上的对位。

## 对“不确定性”的应用历程

说应用历程有点不合适，应该说我们对于“不确定性”的特质的典型应用。
想说的是“不确定性”虽然未必能被理解，但是不妨碍我们可以对它本身进行使用应用。再谈一谈“不确定性”本身的优势。

- **_量子_**

最典型的**量子力学**是“不确定性”在物理学中的体现。不确定性原理告诉我们，无法同时精确知道一个粒子的位置和动量。
而且量子态本身就是概率性的。
粒子可以处于“同时在这里也在那里”的叠加态，被测量的那一刻才“随机”地坍缩到一个确定的状态。

**量子计算**这正是应用了这种“不确定性”来制造非常规的能力。量子比特利用叠加态可以同时进行海量的并行计算。
一些算法巧妙地利用量子态的干涉和概率幅，让“正确”结果的概率被放大，而“错误”结果的概率被抵消，
以远超经典计算机的速度解决某些问题。

经典物理和量子物理并不相溶，但是他们确实同时存在且发挥作用。

- **_AI_**

让生成式AI创作一张图片时，其内部机制是在一个“可能性空间”中进行概率性的采样。
不是去寻找那个“唯一正确”的答案（创造内容来说标准答案本身也不存在），
而是根据它学到的模式，为你生成一个最可能让你满意的同时又带有随机性的结果。

贝叶斯机器学习中，会以置信度来表达对“不确定性”的量化。

强化学习的过程中，会引入大量的随机探索，并且利用这种“不确定”的探索过程中得到的反馈，不断优化自身找到最优解的方法。

“[熵](https://github.com/BBLLMYD/blog/blob/master/blogs/%E6%8A%BD%E8%B1%A1%E4%B9%8B%E4%BA%8E%E2%80%9C%E7%86%B5%E2%80%9D.md)
”也是对不确定性的一种程度量化或者描述。

大量微观层面的“不确定性”互动和累积，最终可能在宏观层面“涌现”出确定的、有序的新结构和规律的过程。

从AI到量子，再到社会和经济，某种角度来说，不确定性是其中的一种原力。

## 正题对于Transformer架构的思考

目前为止，Transformer架构可以认为是大模型构建的事实标准。 Transformer架构的诞生到目前相对成熟的应用延着「论文/理论基础/训练框架迭代/大规模数据验证调试」路径形成。

关于Transformer方方面面的理解都有无限的扩展发散空间，需要有多维全面且融合的知识支撑理解，比如为什么相比其他理论架构为什么Transformer更能落地？到现在还没有其他架构能系统性替代它？架构的演进逻辑有哪些？
细节到注意力机制的理论、并行计算对硬件和算力的极致利用程度、自监督学习和验证等等，可以说需要扎实的数据基础、软件工程、硬件开发效率等等。

在AI产业化或者任何新诞生的产业演化进入到各行业的过程中，偶尔停下来会有不可思议的魔幻感觉。
我觉得大家常常讨论很多的“**涌现**”这个词，是很有魅力很迷人的。或者我更喜欢添加个描述叫它“**自然涌现**”。
“自然”说明它不是被主观强行设计出来的，而是长出来的。或者说是一个整体生命体的迭代步伐。至于步伐去什么方向走向哪里，基本是未知的，但是总是有种种能量去驱动发展。

从原子到分子、从分子聚合到细胞、从细胞聚合到生物、从生物聚合到社会，从生物学角度看每一步都可以看做是一个"涌现"的过程。

到AI模型，当深度学习模型的规模（参数量、数据量、计算量）跨越某个阈值后，系统展现出其小规模版本所不具备的、没有被明确目标训练过的复杂且不是被编程进去的高级能力。
**是系统自己长出来的能力**。


> 维基百科：<br>
>
在深度学习中，transformer（直译为“变换器”）是一种基于多头注意力机制的人工神经网络架构，其中文本被转换为称为词元（token）的数值表示，每个词元通过从词嵌入表中查找转换为一个向量。[1]
在每一层，每个词元都通过并行的多头注意力机制在上下文窗口的范围内与其他（未屏蔽的）词元进行上下文关联，从而放大关键词元的信号并减弱不太重要的词元，从而按输入数据各部分重要性的不同而分配不同的权重。采用该架构的模型主要用于自然语言处理（NLP）与计算机视觉（CV）领域。

### 从工程角度理解神经网络

#### A、如何理解神经网络？

粗略的说符号主义到联结主义的过渡和被接受的过程，是AI发展的基础。本人不是数学专业数据基础也着实一般，只想从工程的角度尝试理解神经结构以及大体的构建的过程。

简单的说符号性质的线性函数，到经过一些加工（称之为激活函数）使得线性函数转为非线性函数，
（为什么要转换成非线性关系呢？因为非线性关系可以完成更复杂的描述和计算，官话叫做**拟合真实数据**）
使得具备了函数对离散的正确答案接近的可能。而不断套激活函数使得原函数拥有了指数级别的变换区间。

```
f(x1,x2) = g(wx1+wx2+b)

# 说明：g()是激活函数，实际上就理解为一段数据逻辑处理（方法调用）即可
```

第一步实际上形成了初步的非线性转换，也就具备了非线性基础。而由于激活函数可以不断使用，**将已有的函数结构作为整体再次添加参数进行线性变换同时做激活操作
**，就得到了+1维度的转换。

```
f(x1,x2) = g(w3 g(wx1+wx2+b)+b2)
```

循环往复，输入数据的x1,x2，就可以经过多层线性变换和激活函数形成输出。这个过程的名词叫做“前向传播”

每一个维度视为一个维度的神经元，那么整个函数变换形成的结构，就可以视为神经网络。

也就是符号主义 -> 联结主义的过渡。

#### B、如何计算神经网络上的参数

```
L(w,b) = **x1#y1#*乱写的*x2**y2*

L就是损失函数，要知道的是损失函数的输入内容和输出值是核心，计算过程可变换
```

真实数据与预测数据的误差，叫做损失函数。形式上可以很多方式（效果好为佳）来表示比如均方误差。我们的目标是让这个误差达到可能的尽量小。
就是找到让上面函数中，可以让损失函数值最小的w和b的值。在不停随机尝试和计算的过程中，通过寻找一个线性函数来拟合x和y的关系，过程就称为线性回归。
从右向左（从输出层到输入层）依次计算求复合函数逐步更新每一层的参数（就是w和b）,直到把所有层的参数都更新一遍，因为从右向左的过程每算一层的参数再再右一层都会用到，所以整个过程称之为反向传播。

这样传播一次，就构成了当前神经网络的一次训练。不断进行前向和反向传播，整个神经网络就经过多轮训练，里面的参数都会逐渐变化，直到让损失函数足够小，也就调整到了符合我们预期（更能接近正确答案）的函数。

#### C、如何让模型鲁棒性更好

让模型鲁棒性更好，其实就是让模型在宽泛的数据集中有更稳定、优质的表现。

- 从训练数据角度
  更好的拟合能力，工程上可以理解为让模型本身有更强的鲁棒性，实际就是更好的参数。如何得到更好的参数，可以通过优质的训练样本来做到，如何定义更好的训练样本呢？
  首先绝对不是精纯的正确数据，这样会让模型过拟合，导致只能在样本数据产生好的效果。优质的样本可以理解为结合噪声、多维调整等手段，来不断调试，让模型更加“智能”和“鲁棒”的的过程。

- 从参数计算过程角度
  在减小损失函数的过程里，通过引入“超参数”的概念平衡损失函数和模型参数的关系，防止参数过大，其实还是防止过拟合。

- 在参数结果中进行调整
  为了避免对某些关键参数（或者大参数）上的过渡依赖，**类似的工程中常见的数据倾斜或者热key**，是一种不均衡的体现，这样会放大系统性的风险。
  对于系统来说更可控就是要对一些重量级参数更可用，可以删掉极值或者控制其发挥作用的频率，防止过渡依赖。

---
`前向传播可以理解为模型推理的过程，反向传播可以理解为模型学习的过程。
`
---

整个模型训练过程的预期状态和步骤大体是如上述，但是实际过程中，可能出现各种预期之外的问题，和我们工程的系统上线类似也会避免不了出现没有意想到的问题。
比如网络越深的时候，梯度反向传播时会越来越小，导致参数更新困难，所谓梯度消失；|可能需要结合残差网络等方式减少深层网络的影响
比如参数的调整幅度失控，导致梯度数值越来越大，叫做梯度爆炸；|比如需要结合梯度裁剪，权重初始化
陷入局部最优或者来回震荡的时候，叫做收敛速度过慢；|比如结合动量法等方式减少震荡
数据规模过大导致计算开销过大难以完成训练；|分批次训练降低单次开销

实际中要结合的优化策略和手段才是核心重点。好像写hello world和应对企业级工程的开发都算会编程，但中间可能差着一个维度。

#### D、一些传统基础操作：矩阵|CNN|词嵌|RNN

将函数从线性和非线性的写法，抽象和矩阵的形式，这样可以充分的利用硬件GPU的计算能力，加速整个推理和训练的过程，实际矩阵运算也更适合GPU的架构。

在静态图像处理领域中，为了避免不能理解图像的局部模式的情况（比如图像整体变暗或者位置移动，产生整体的神经数据变化），利用卷积运算（可以理解为全连接参数/函数变换为卷积核的形式）做识别计算。
而对卷积层产生的特征图像的效率优化的动作发生在后面叫做“池化层”，主要做一些特征降维、减少计算量的动作。整体新的计算和多层协作方式称为卷积神经网络CNN。

在将数据内容转换为计算机理解的内容的过程中，我们称之为向量化，向量化的过程经过高低维度平衡等一系列迭代，到目前最普适的是以“词嵌入”的方式得到特征（特征经过模型训练出来），其中词和词之间的相似性，
用多个向量之间的点积或者余弦相似度来表示，这就将向量之间对应的自然语言之间的联系准备了以数据学公式计算的基础。

整个向量组成的矩阵就叫做嵌入矩阵，有了词向量后，在经典神经网络计算的基础上，多一个前一时刻的隐藏状态在词之间传递，粗略的理解这样形成的新的结构就叫做循环神经网络RNN。
这种方式存在的问题比如无法捕捉远距离长期依赖，需要按顺序处理无法进行并行计算。虽然过程有LSTM等方法做针对性优化，但是本质还是在顺序的基础上，没能根本的解决问题。
所以实际以上思路目前都被迭代，已经属于传统方案在当前的Transformer架构中被取代了。


#### E、Transformer架构的方案

Transformer架构中，给了每个词向量一个位置编码，再通过预先训练出的权重值经过一系列计算，
得到其他词的上下文信息。由此构成一组新的词向量（包括位置信息和其他词的上下文，有点类似于空间换时间的思想，通过一系列计算解决了长距离长依赖的问题）。
这个过程做的事情就是形成Attention注意力机制的基础。多头注意力就可以理解为经过多次权重矩阵的计算生成多组计算结果，然后经过多次学习后得到的向量结果再进行拼接，形成了更灵活的多头注意力。

<br>
<div align=center><img src="https://github.com/BBLLMYD/blog/blob/master/images/shang/shang333.jpg?raw=true" width="456"></div>
<br>
