## 一、对传统编程范式的反思

> **程序 = 数据结构 + 算法**

这个公式是在学习软件工程专业时的第一课内容可以说是认知基础，是一种基准范式。而且后面的学习内容也基本都是在这上展开的。
数据结构是组织和存储数据，而算法是处理数据的步骤和方法。

直到工作了多年，我仍然认为这个公式是正确的，且随着多年工作经验的累积，每想起来这个公式，**愈发认为这个公式的描述是十分精准并且干练的。**

但随着接触AI的内容越多，关于这个公式，逐渐产生了不少疑问或怀疑。

因为大部分的**工程类程序员，工作的上下文都是在一个严肃或者相对严肃的场景**。

怎么说严肃呢，比如支付业务的流量直接涉及到金钱离资损风险近，所以各个环节每笔流量都需要做好各种可能性的谨慎应对，做好全面的兜底保障。
同时对于数据的任何变更都要做清晰，且具备**可解释和追溯**的能力。用支付业务举例体感更直接，实际任何业务都有一样对**确定性**的要求。

总的说大多数工程类程序员其实追求的，无论是鲁棒性或者扩展性性能等，都是以确定性为终极目标来进行系统建设的。

> 我们知道AI作为主体某种程度上是以随机和概率为基础，那么这个公式在AI为基础主体下的编程还适用吗？

我觉得一定程度上，以程序=数据结构+算法公式为基础的编程范式，因为AI的涌现，会大面积受到影响，新的编码范式正在加速形成。

可以认为两种范式一直是共存的，从分布上来说只是大多数工业级企业工程开发，都是以传统范式为驱动指导的。
而AI时代的到来将这种情形扩大。

而我想说的是并不是说传统的编码范式收到了挑战，而是随着AI时代的来到新的范式给了我们新的选择和路径。
就像**传统物理和量子物理并不相溶，但它们是共存共生的。**

新的范式还没有统一的标准，我们可以结合元素先将它视作：

> 程序 = 数据结构 + 算法 + 模型 + 数据分布 + 不确定性控制

## 二、关于AI应用的延伸思考

那为什么在AI的相关的内容的理解和应用的过程中，对上述公式频频产生疑问呢？我认为很重要的一点就是AI的工作机制充满了工程视角的“不确定性”。

很多认知对AI的基础认识是随机+概率，从工程角度来看，概率和随机是AI的原生工具，而在这基础之上有复杂的用来稳定结构的目标优化的方法设计和行为。
其实从某种程度上来说或者生物本身也是一种概率系统。

那在严肃场景AI为基础的智能体应用，到底应该怎么落地？很长一段时间，我都比较保守的认为AI更适合做一些相对保守且稳妥的工作，如果把对数据操作分为
**读/写**的话，我总觉得以读为基础的工作更适合AI，
实际上也确实很多应用都是围绕读的，比如各种知识库理解的对话应用。因为如果让AI做写操作的话，工程视角总会自然认为这种动作充满了不确定性。

其实不确定性，或者说是不稳定性在**AI作为基础主体**的情况下，是客观存在的。那么**问题就来到如何让AI的写动作，变得可控**
，可控的具体包括一下特性比如精准、稳定、可追溯、可解释、可回滚等等。
2026春节火热的千问请大家奶茶的营销活动，是我印象里第一个与C端用户直接交互的有“订单”产生的模式。不同于比如广告业务实际背后也是AI支撑以及数据回写的流程，这种业务里AI的角色并不直接和用户直观的产生交互体验上的对位。

## 三、我们对“不确定性”的应用历程

说应用历程有点不合适，应该说我们对于“不确定性”的特质的典型应用。
想说的是“不确定性”虽然未必能被理解，但是不妨碍我们可以对它本身进行使用应用。再谈一谈“不确定性”本身的优势。

- **_量子_**

最典型的**量子力学**是“不确定性”在物理学中的体现。不确定性原理告诉我们，无法同时精确知道一个粒子的位置和动量。
而且量子态本身就是概率性的。
粒子可以处于“同时在这里也在那里”的叠加态，被测量的那一刻才“随机”地坍缩到一个确定的状态。

**量子计算**这正是应用了这种“不确定性”来制造非常规的能力。量子比特利用叠加态可以同时进行海量的并行计算。
一些算法巧妙地利用量子态的干涉和概率幅，让“正确”结果的概率被放大，而“错误”结果的概率被抵消，
以远超经典计算机的速度解决某些问题。

经典物理和量子物理并不相溶，但是他们确实同时存在且发挥作用。

- **_AI_**

让生成式AI创作一张图片时，其内部机制是在一个“可能性空间”中进行概率性的采样。
不是去寻找那个“唯一正确”的答案（创造内容来说标准答案本身也不存在），
而是根据它学到的模式，为你生成一个最可能让你满意的同时又带有随机性的结果。

贝叶斯机器学习中，会以置信度来表达对“不确定性”的量化。

强化学习的过程中，会引入大量的随机探索，并且利用这种“不确定”的探索过程中得到的反馈，不断优化自身找到最优解的方法。

“[熵](https://github.com/BBLLMYD/blog/blob/master/blogs/%E6%8A%BD%E8%B1%A1%E4%B9%8B%E4%BA%8E%E2%80%9C%E7%86%B5%E2%80%9D.md)
”也是对不确定性的一种程度量化或者描述。

大量微观层面的“不确定性”互动和累积，最终可能在宏观层面“涌现”出确定的、有序的新结构和规律的过程。

从AI到量子，再到社会和经济，某种角度来说，不确定性是其中的一种原力。

## 四、正题对于Transformer架构的思考

目前为止，Transformer架构可以认为是大模型构建的事实标准。 Transformer架构的诞生到目前相对成熟的应用延着「论文/理论基础/训练框架迭代/大规模数据验证调试」路径形成。

关于Transformer方方面面的理解都有无限的扩展发散空间，需要有多维全面且融合的知识支撑理解，比如为什么相比其他理论架构为什么Transformer更能落地？到现在还没有其他架构能系统性替代它？架构的演进逻辑有哪些？
细节到注意力机制的理论、并行计算对硬件和算力的极致利用程度、自监督学习和验证等等，可以说需要扎实的数据基础、软件工程、硬件开发效率等等。

在AI产业化或者任何新诞生的产业演化进入到各行业的过程中，偶尔停下来会有不可思议的魔幻感觉。
我觉得大家常常讨论很多的“**涌现**”这个词，是很有魅力很迷人的。或者我更喜欢添加个描述叫它“**自然涌现**”。
“自然”说明它不是被主观强行设计出来的，而是长出来的。或者说是一个整体生命体的迭代步伐。至于步伐去什么方向走向哪里，基本是未知的，但是总是有种种能量去驱动发展。

从原子到分子、从分子聚合到细胞、从细胞聚合到生物、从生物聚合到社会，从生物学角度看每一步都可以看做是一个"涌现"的过程。

到AI模型，当深度学习模型的规模（参数量、数据量、计算量）跨越某个阈值后，系统展现出其小规模版本所不具备的、没有被明确目标训练过的复杂且不是被编程进去的高级能力。
**是系统自己长出来的能力**。


> 维基百科：<br>
>
在深度学习中，transformer（直译为“变换器”）是一种基于多头注意力机制的人工神经网络架构，其中文本被转换为称为词元（token）的数值表示，每个词元通过从词嵌入表中查找转换为一个向量。[1]
>
在每一层，每个词元都通过并行的多头注意力机制在上下文窗口的范围内与其他（未屏蔽的）词元进行上下文关联，从而放大关键词元的信号并减弱不太重要的词元，从而按输入数据各部分重要性的不同而分配不同的权重。采用该架构的模型主要用于自然语言处理（NLP）与计算机视觉（CV）领域。

### A、从工程角度如何理解神经网络？

可以大体的认为**符号主义到联结主义的过渡和被接受的过程**，是AI发展的基础。本人不是数学专业数据基础也着实一般，只想从工程的角度尝试理解神经结构以及大体的构建的过程。

简单的说符号性质的线性函数，到经过一些加工（称之为激活函数，激活函数会以线性模式保留正相关的关联信息）使得线性函数转为非线性函数，
（为什么要转换成非线性关系呢？因为非线性关系可以完成更复杂的描述和计算，官话叫做**拟合真实数据**）
使得具备了函数对离散的正确答案接近的可能。而不断套激活函数使得原函数拥有了指数级别的变换区间。

```
f(x1,x2) = g(wx1+wx2+b)

# 说明：g()是激活函数，实际上就理解为一段数据逻辑处理（方法调用）即可
```

第一步实际上形成了初步的非线性转换，也就具备了非线性基础。而由于激活函数可以不断使用，**将已有的函数结构作为整体再次添加参数进行线性变换同时做激活操作
**，就得到了+1维度的转换。

```
f(x1,x2) = g(w3 g(wx1+wx2+b)+b2)
```

循环往复，输入数据的x1,x2，就可以经过多层线性变换和激活函数形成输出。这个过程的名词叫做“前向传播”

每一个维度视为一个维度的神经元，那么整个函数变换形成的结构，就可以视为神经网络。

也就是符号主义 -> 联结主义的过渡。

---

### B、如何计算神经网络上的参数

```
L(w,b) = **x1#y1#*乱写的*x2**y2*

L就是损失函数，要知道的是损失函数的输入内容和输出值是核心，计算过程可变换，通常以让损失函数输出值尽量小为目标
```

真实数据与预测数据的误差，叫做损失函数。形式上可以很多方式（效果好为佳）来表示比如均方误差。我们的目标是让这个误差达到可能的尽量小。
就是找到让上面函数中，可以让损失函数值最小的w和b的值。在不停随机尝试和计算的过程中，通过寻找一个线性函数来拟合x和y的关系，过程就称为线性回归。
从右向左（从输出层到输入层）依次计算求复合函数逐步更新每一层的参数（就是w和b）,直到把所有层的参数都更新一遍，因为从右向左的过程每算一层的参数再再右一层都会用到，所以整个过程称之为反向传播。

这样传播一次，就构成了当前神经网络的一次训练。不断进行前向和反向传播，整个神经网络就经过多轮训练，里面的参数都会逐渐变化，直到让损失函数足够小，也就调整到了符合我们预期（更能接近正确答案）的函数。

深度学习就是指基于深层（由单层到多层到深层多维立体）的神经网络模型的机器学习。

---

### C、如何让模型鲁棒性更强

让模型鲁棒性更好，其实就是让模型在宽泛的数据集中有更稳定、优质的表现。

- 从训练数据角度
  更好的拟合能力，工程上可以理解为让模型本身有更强的鲁棒性，实际就是更好的参数。如何得到更好的参数，可以通过优质的训练样本来做到，如何定义更好的训练样本呢？
  首先绝对不是精纯的正确数据，这样会让模型过拟合，导致只能在样本数据产生好的效果。优质的样本可以理解为结合噪声、多维调整等手段，来不断调试，让模型更加“智能”和“鲁棒”的的过程。

- 从参数计算过程角度
  在减小损失函数的过程里，通过引入“超参数”的概念平衡损失函数和模型参数的关系，防止参数过大，其实还是防止过拟合。

- 在参数结果中进行调整
  为了避免对某些关键参数（或者大参数）上的过渡依赖，**类似的工程中常见的数据倾斜或者热key**，是一种不均衡的体现，这样会放大系统性的风险。
  对于系统来说更可控就是要对一些重量级参数更可用，可以删掉极值或者控制其发挥作用的频率，防止过渡依赖。

> 前向传播可以理解为模型推理的过程，反向传播可以理解为模型学习的过程。

整个模型训练过程的预期状态和步骤大体是如上述，但是实际过程中，可能出现各种预期之外的问题，和我们工程的系统上线类似也会避免不了出现没有意想到的问题。
比如网络越深的时候，梯度反向传播时会越来越小，导致参数更新困难，所谓梯度消失；|可能需要结合残差网络等方式减少深层网络的影响
比如参数的调整幅度失控，导致梯度数值越来越大，叫做梯度爆炸；|比如需要结合梯度裁剪，权重初始化
陷入局部最优或者来回震荡的时候，叫做收敛速度过慢；|比如结合动量法等方式减少震荡
数据规模过大导致计算开销过大难以完成训练；|分批次训练降低单次开销

实际中要结合的优化策略和手段才是核心重点。好像写hello world和应对企业级工程的开发都算会编程，但中间可能差着一个维度。

---

### D、一些传统基础操作：矩阵|CNN|词嵌|RNN

将函数从线性和非线性的写法，抽象和矩阵的形式，这样可以充分的利用硬件GPU的计算能力，加速整个推理和训练的过程，实际矩阵运算也更适合GPU的架构。

在静态图像处理领域中，为了避免不能理解图像的局部模式的情况（比如图像整体变暗或者位置移动，产生整体的神经数据变化），利用卷积运算（可以理解为全连接参数/函数变换为卷积核的形式）做识别计算。
而对卷积层产生的特征图像的效率优化的动作发生在后面叫做“池化层”，主要做一些特征降维、减少计算量的动作。整体新的计算和多层协作方式称为卷积神经网络CNN，通常用于图像数据处理。

在将数据内容转换为计算机理解的内容的过程中，我们称之为向量化，向量化的过程经过高低维度平衡等一系列迭代，到目前最普适的是以“词嵌入”的方式得到特征（特征经过模型训练出来），其中词和词之间的相似性，
用多个向量之间的点积或者余弦相似度来表示，这就将向量之间对应的自然语言之间的联系准备了以数据学公式计算的基础。

整个向量组成的矩阵就叫做嵌入矩阵，有了词向量后，在经典神经网络计算的基础上，多一个前一时刻的隐藏状态在词之间传递，粗略的理解这样形成的新的结构就叫做循环神经网络RNN，用于序列数据处理。
这种方式存在的问题比如无法捕捉远距离长期依赖，需要按顺序处理无法进行并行计算。虽然过程有(
LSTM)[https://en.wikipedia.org/wiki/Long_short-term_memory]等方法做针对优化，但是本质还是在顺序的基础上，解决的程度有限没能根本的解决问题。
所以实际以上思路目前都被迭代，已经属于传统方案在当前的Transformer架构中被取代了。

---

### E、Transformer的架构方案

(
Transformer论文)[https://arxiv.org/abs/1706.03762?utm_source=chatgpt.com]诞生在2017年，是一种特殊深度学习模型结构，核心是引入自注意力机制。Transformer架构中，给了每个词向量一个**位置编码**，再通过预先训练出的权重值经过一系列计算，
得到其他词的上下文信息。
由此组合成一组新的词向量（包括位置信息和其他与当前向量相关的上下文，有点类似空间换时间，但实际不止是以空间置换时间核心的是多维计算，而换的而不只是时间而是信息关联和表达能力，同时通过一系列计算解决了长距离长依赖的问题）。
这个过程做的事情就是形成注意力机制的基础。多头注意力就可以理解为经过多次权重矩阵的计算生成多组计算结果，
在这一层称为隐藏层，把数据信息指数级的糅合信息叠加动态增强，
然后经过多次学习后得到的向量结果再进行拼接，形成了更灵活强大的多头注意力。

<br>
<div align=center><img src="https://github.com/BBLLMYD/blog/blob/master/images/15/Transformer4.png?raw=true" width="678"></div>
<br>



理解了Transformer的工作机制，那么看下来Transformer的优势包括

1. 并行计算能力，更充分利用硬件资源
2. 捕捉长距离依赖，自注意力机制让每个位置的输入可以与其他信息位置进行交互和向量存留
3. 扩展性

---

### F、Transformer架构的经典变式

---

### G、从工程角度理解大模型应用的常见名词和操作

-

权重（Weight）：只要是“可训练的”，都可以泛指叫参数。权重是参数里最核心的一类，用来做“加权求和”，权重占90%以上参数数量，偏置和归一化参数相比很少。所以加载模型权重约等于加载模型参数。开源模型通常指的是开放了权重而不是开放训练代码和训练数据。

- 大模型（Large Model）：参数量特别大，无明确标准，可以认为参数量≥10B（百亿）是通常意义上的大模型，参数量≥100B（千亿）是确定的大模型
- LLM（Large Language Model）：用于自然语言处理的模型叫大语言模型。
- 训练（Training）：调整功能参数的过程。
- 预训练（Pretraining）：预先训练好一个基础模型。
- 微调（Fine-Tuning）：基于预训练好的模型继续训练，让模型学会比如某领域知识或者任务的过程。
- 推理（Inference）：调整好参数后，根据输入内容输出结果的过程。
- 涌现（Emergence）：量变引起质变出现之前没有过的能力且不是预先设计好的能力的现象。
- Token：模型的输入输出最小粒度的基础计量单位
- 温度：控制模型随机性程度的参数
- Top-K：从概率最高的K的结果中选择
- RAG（Retrieval-Augmented Generation）：检索增强生成，给模型外挂信息
- AGI（Artificial General Intelligence）：一个无所不能的AI
- MCP（Model Context Protocol）：一种标准化协议（模型调用工具的交互场景）
- A2A（Agent-to-Agent Protocol）：一种标准化协议（Agent之间通信协作的场景）
- 蒸馏（Distillation）：用参数量较大的大模型指导参数量较小的小模型
- 枝剪（Pruning）：删除模型中的不重要的神经元，目的提速
- LoRA（Low-Rank Adaptation）：更低成本微调的一种方式
- RLHF：通过人类反馈强化学习
- vLLM：提升大语言模型推理速度的推理引擎