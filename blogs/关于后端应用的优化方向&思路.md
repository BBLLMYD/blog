首先要明确当前应用在当下阶段优化的必要性是不是真的存在，此时应用是否看到了瓶颈，或者说调优是一定要有目标来驱动，杜绝盲目的优化。
如果当前应用是以计算为主的CPU密集的应用，自然就比较需要关注CPU吞吐率情况；
如果是IO密集的应用，自然就比较需要关注影响IO瓶颈和稳定性相关的指标；
如果追求低延时的响应应用，就需要尽量避免出现停顿用户线程的情况；
如果是内存紧张的环境，内存占用也可以是我们优化方向。
不同类型的应用都有优化的空间和方向，但是早期健康的应用程序的瓶颈还不清晰，无法准确找到优化方向， 
这时候做优化是不合理并且是很难做出真正有效的优化的。

> "过早优化是万恶之源"

虽然过早的优化是不建议的，**或者说在合适的时间做合适的事情才会有更好的结果**。
明确目标而不偏离，更容易生成合理的投入产出比。
随着时间累计，有的应用总是会出现越来越多的数据要处理、越来越大的流量在出入、越来越稳定和快的系统响应需求等等。
此时伴随着系统不停迭代，性能层面的优化是自然也是必然要做的。
对业务模型有着深刻的理解，可以在系统的迭代和发展的方向的把控上更加合理，节省成本少走弯路。
对相关基础知识扎实的掌握，能够做出有效的，对当前应用特点有针对性的性能上的优化。

在已有的业务模型基础上，从宏观到微观的优化方向和思路，
大多数优化的本质还是**从当下场景出发 做最适合的选择**，
真正做到有针对的创新性的优化还是偏少数的，毕竟定制造轮子或是试错的成本都是极高的。

下面这篇按照一个**自底向上**的视角来讨论一下与后台应用系统相关的一些内容：

- [1. 从"OS & 网络"等资源出发：]()
- [2. 从"应用"本身出发：]()
- [3. 从系统所处阶段看稳定性手段]()
- [4. 总结]()

---

### 1.从"OS & 网络"出发

绝大部分的后台应用都是部署在Linux系统上，想要最大化的合理利用系统资源，
对Linux下的CPU、内存、I/O、网络等方面的工作机制和原理需要有一定程度的了解，
因为我们的各种应用服务大多数的瓶颈总是离不开这些资源，
在关于[负载]()主题讨论的文章中有大致介绍过关于操作系统下资源的负载情况的模块，
这里我们来更有针对性的讨论一些关于应用在系统资源层面的导致的瓶颈的类别原因及相应优化。
只有清楚的知道应用程序的特点以及清楚与之耦合密切的资源，这样才能准确有效的提升系统的运行效率，提高系统的瓶颈。

#### 1.1 关于CPU

对CPU首先要避免在程序里出现死锁死循环等严重异常情况发生，
此时不只是应用程序运行的问题，严重的情况可能还会拖垮其他进程。
**CPU正常使用瓶颈其实大多是来自于程序计算的算法复杂度、CPU的使用率异常以及对CPU不合理的频繁上下文切换。**
当然也有在应用层针对CPU的特性做出高效利用的设计创新方式，
如[高效队列Disruptor]()就是通过应用层的设计最大程度的提升了CPU Cache的利用率，大大提高应用程序的吞吐能力。
（Disruptor中的一些设计思路和方式在另一片文章中有讨论）
根据任务的不同，CPU的上下文切换就可以分为几个不同的场景：进程上下文切换、线程上下文切换以及中断上下文切换。
我们主要谈一谈线程上下文切换，线程是调度的基本单位，而进程则是资源拥有的基本单位。
**内核中的任务调度，实际上的调度对象是线程；而进程是给线程提供了虚拟内存、全局变量等资源。**
所以同进程内的线程上下文切换切换，要比多进程间的切换消耗更少的资源，这也是合理资源管理的一个体现。
碰到上下文切换次数过多的问题时，可以借助vmstat、pidstat等工具来排查进而解决性能问题

<br>                                     
<div align=center><img src="https://github.com/BBLLMYD/blog/blob/master/images/10/1001.png?raw=true" alt="CPU性能相关" width="837"></div>
<div align=center>CPU性能相关</div>
<br>

上图中覆盖了一部分关于CPU性能的优化方向和方法，
但是也提到了其实有一个原则就是避免过早优化，一方面过早优化可能会增加不必要的复杂度，另一方面过早阶段无论是业务还是场景都处于相对高频变化的阶段，很难看清更远。
好的优化一定是是动态的、在相对都比较可控的情况下逐步完善的。

<br>

---

#### 1.2 关于OS的内存

计算机中，只有内核才可以直接访问物理内存，Linux为用户进程分配独立同时连续的虚拟地址空间，进程访问的是虚拟内存，
这样各个进程的内存可以互相独立而且看起来是互不干扰的。
虚拟内存的部分又分为内核空间和用户空间，而且用户空间内存又根据存储数据的类别分为五个段（栈、文件映射、堆、数据段、只读段），
其中栈、文件映射和堆通常是用来给程序动态分配使用。
进程的虚拟内存需要通过页表，由系统映射为物理内存。虚拟内存加起来的地址空间一定比物理内存大很多，

上面提到的用户虚拟内存中的堆内存由应用程序自己来分配和管理，这些堆内存并不会被系统自动释放，除非程序退出。
需要在应用程序中明确调用库函数free()来释放它们。
**如果应用程序没有正确释放堆内存，就会造成内存泄漏，
泄漏的内存不仅应用程序自己不能访问，在OOM或者kill掉进程之前操作系统也不能把它们再次分配给其他应用来使用。**
所以在应用程序中一定要尽量避免内存泄漏的情况发生，如果一旦发生，要能够借助合适的工具如memleak、valgrind及时检测出原因。

Swap把一块磁盘空间或者一个本地文件，当成内存来使用。这样系统的可用内存好像就变大了，但是由于需要磁盘读写，换入换出时的访问速度可能会受到影响。
这是**时间换空间的思想的一种应用。但是实际上大多数的场景我们的程序更需要的都是时间，不仅不需要Swap机制，甚至还要避免带来的性能影响。**
如Java程序在gc的时候，要遍历所有用到的堆内存，如果有Swap的部分，遍历的时候肯能会出现磁盘I/O。
所以开启Swap其实会严重影响性能，大多数情况都需要关闭Swap机制避免性能的损失。
smem --sort swap命令可以看到各个进程的swap用量，如果有过高的情况可能就需要注意性能了。
需要Linux提供一些机制来应对内存不足的情况，如Buffer和Cache的回收、交换分区Swap以及OOM等机制，Linux系统中的缓冲命中率足够高的话也可以极大的提升I/O性能和吞吐率，但是这些都是由操作系统直接管理的。

在下面的图中总结了一些指标分类和相关工具，以及一些优化上的一些原则和思路。

<br>                                     
<div align=center><img src="https://github.com/BBLLMYD/blog/blob/master/images/10/1002.png?raw=true" alt="内存性能相关" width="837"></div>
<div align=center>内存性能相关</div>
<br>

--- 

#### 1.3 关于I/O & 网络

无论是磁盘IO或是网络的IO，在Linux下都被已经抽象成了文件，
都是通过统一的VFS接口来进行访问，而对文件读写方式的差异又将I/O操作的种类分为了不同类别，如：

1. ***缓冲 / 非缓冲IO***

- 缓冲：利用标准函数库加速文件的访问
- 非缓冲：直接通过系统调用访问文件
           
2. ***直接 / 非直接IO***

- 直接：指跳过操作系统的页缓存，直接跟文件系统交互来访问文件
- 非直接：先要经过系统的页缓存，然后由内核或额外的系统调用
- 裸IO：数据库场景，跳过文件系统直接读写磁盘

3. ***阻塞 / 非阻塞IO***

- 阻塞：应用程序执做IO操作后没有直接获得响应时，阻塞当前线程
- 非阻塞：应用程序执做IO操作后没有直接获得响应时，继续去执行其他任务，线程资源不进入阻塞状态

4. ***同步 / 非同步IO***

- 同步：线程同步获取IO操作的结果
- 异步：线程IO操作后完全不等待，等待系统回掉结果通知给应用程序，需要依赖系统内核的实现

（关于网络IO部分也可以看一下我的[另一篇](https://github.com/BBLLMYD/blog/blob/master/blogs/%E6%8A%BD%E8%B1%A1%E4%B9%8B%E4%BA%8E%E2%80%9CIO%E2%80%9D.md)相关讨论，会更具体和全面一些）

关于磁盘IO，iostat是一个常用的工具，可以提供磁盘的使用率、IOPS、吞吐量等性能指标，这些指标的数据实际是来源与/proc/diskstats。
从进程的角度可以用pidstat，iotop等工具来观察。
为了提高磁盘IO的效率，Linux也会利用内存中缓存来适配协调磁盘和CPU的速度差值，这对顺序读写的场景更友好。

在网络相关的性能指标中，带宽、吞吐量、延时、IO模型等这些都需要在参考范围内，
对于上层的业务应用来讲很重要也需要做到一定程度的可控，要一定程度的知晓应用所在环境的网络上下文的全貌。
**带宽上要能根据需要满足的业务预估量，提出合理的带宽需求；
应用的工作模型上要选择最适合的IO模型，这样才能更好的处理请求和最大化利用资源。**

- 带宽：链路的最大传输速率，单位是 b/s（比特 / 秒）

- 吞吐量：没有丢包时的最大数据传输速率（B/s（字节 / 秒）），吞吐量/带宽代表着该网络链路的使用率。

- 延时：一个数据包往返所需时间，或者TCP握手延时，网络延时是一个相对宽泛的概念，要结合具体场景。
   
- ... ...

深谙原理才能更自由的利用工具，在OS层面的优化需要对OS有一定的认知，
甚至要有从Linux内核日志找出问题原因的能力，因为这里是来自根源的没有经过包装的"一手"信息。

众所周知网络模型是分层的，这样分层的特性可以使得职责分离，做更有针对性的优化。
关于基于TCP协议的应用和优化方向等相关的内容，可以结合我的[另一篇](https://github.com/BBLLMYD/blog/blob/master/blogs/%E5%85%B3%E4%BA%8E%E5%9F%BA%E4%BA%8ETCP-IP%E7%9A%84%E4%B8%8A%E5%B1%82%E5%BA%94%E7%94%A8.md)更具体的讨论。
我认为主要的思路一个是从应用层面出发，选择最适合的IO
是对网络连接状态的相关内容要有合理的把控。

<br>                                     
<div align=center><img src="https://github.com/BBLLMYD/blog/blob/master/images/10/1003.png?raw=true" alt="IO & 网络相关" width="837"></div>
<div align=center>IO & 网络相关相关</div>
<br>

--- 

### 2.从"应用"本身出发

####  应用所属类型与"演进&优化"方向的关系

比如KafKa等消息队列系统的实现通常会对磁盘I/O的和网络I/O的交互等方向多花心思来设计，因为程序需要和磁盘和网络打交道的占比是很大的，IOPS决定着中间件的性能和特性。
比如在流媒体类似的场景下的顺序读写较多，吞吐量指标此时更能够反映系统的整体性能。
数据库场景下，多数是对文件进行随机读写，此时 IOPS 更能反映出整体性能。

比如redis作为一个内存数据库，如何进行合理高效内存管理以及对内存如何高效的利用自然就是重点。
再比如一个交易系统设计的重点一定是如何保证系统事务的一致性可靠性为重心，在这前提基础上才再去设计如何提升性能等；
一个IM系统设计的重点一定是如何提升网络链接的可靠性为重心，在这基础上又怎么合理高效可靠的管理链接，以及满足业务上拓展等。

当然是分布式的存储系统也都会涉及到对CAP等方面的实现机制，分布式的业务系统也都会涉及一些如熔断、流控、安全、监控、幂等性等通用的机制。
但是从具体系统实现的角度来看，无论是各种中间件还是业务系统，都有着会偏重的方向。要清楚知道你的应用是属于一个什么类型，这样系统才有可能有针对性在合理的方向上演进和优化。
比如CPU密集的计算型应用，就是如何最大化提升CPU吞吐，程序设计的方向也是如何降低算法的时间复杂度，甚至如何能够合理高效的利用CPU缓存。
如果是IO密集型的网络应用或者存储系统，就是如何设计或者选型合理高效的IO模型，如何让应用程序的线程资源在IO密集的情况下能够最大化的利用而不浪费等等。

总的来说性能上的优化点，其实是比较直接的，因为可能会直接影响用户体验。
**性能优化更像打靶子，定位瓶颈后就目标明确，剩下的工作就是以优化缺陷和找到高效的优化手段为主。
而一些让系统长期性的可持续的建设经常会被选择性的忽略，**
或者说有时候碍于时间等资源就不得不把良好的可扩展等等的设计权重放轻，但是可能"早晚是要还的"，
这些内容都应该在一个可控范围内，起码做到心理有数，能够不至于一地鸡毛，这样成本可能就太大了。

---

### 3.从系统所处阶段看稳定性手段

1. ***开发阶段-代码质量***

- 代码的扩展性、可读性、性能意识，持续集成
- Code Review

2. ***运行阶段***
    
- 隔离、限流、熔断、降级
- 监控、告警、链路追踪
- 监控系统中的曲线毛刺原因、GC、负载等方面

3. ***故障阶段***

- 故障预案恢复
- 故障排查工具：包括但不限于上边提到的CPU、内存、磁盘、网络等方面的排查工具，熟悉原理才能更有效的利用工具
- 线上出现的问题要尽量复现
    
---

### 4.总结

上面提到"过早的优化是万恶之源"，我认为这话肯定有道理但其实也存在一点点偏激。
或者说即使不过早的做出优化行为，但是系统迭代周期的可控性也一定需要有一个把控的，
**也就是说可以不过早做优化，但对整体的演进和宏观上所处阶段一定要可控，这样才能心里有底。**
至少不可以等到一地鸡毛了才想起来优化这件事，这更是要杜绝的。

在一个针对性的优化行为中，"木桶原理"往往会告诉我们系统的瓶颈在哪，我们的目标会很清晰不会偏离，
这也是阶段性优化的一个好处。

性能优化不同于设计上的优化，性能优化比较直接直观，找到瓶颈后可以有的放矢，指哪打哪。
但设计上的优化就显得比较虚了，并且不太容易量化，**但是良好的设计是能决定你的应用的长跑能力**。还是要做一个能够尽量看得远的 owner。

在实际应用中上面提到的系统和网络等基础设施中往往各个指标都是紧密相关的，当真的出现性能瓶颈，
一般从一个点顺藤摸瓜去找本质的原因，这需要在基础知识上有一定的深度和广度，才能尽可能快速准确定位到瓶颈，再快速准确的进行优化。
free、top、vmstat、pidstat、netstat是一些基础又强大的工具，而且输出的信息覆盖面比较大，可参考的维度也比较灵活。
PS：过于孤立的某一项指标经常不太能构成有针对性的意义，需要结合多维度才能更清晰立体的定位，
虽然应用程序可能有偏重热点的组件，但计算机工作和组成毕竟不是由于某个单一的组件来完成而是各部分协作工作的。


